# Shopware Vector Search - Flexible Embedding Setup

## √úbersicht

Dieses Plugin unterst√ºtzt **zwei Modi** f√ºr die Embedding-Generierung:

1. **üåê External Embedding Service** - Nutzt einen separaten Embedding-Server
2. **ü§ñ Direct OpenAI API** - Direkte Verbindung zur OpenAI API

## Installation

### 1. Dependencies installieren
```bash
cd ShopwareVectorSearch/
composer install
```

### 2. Plugin in Shopware installieren
```bash
# Plugin aktivieren
bin/console plugin:install --activate ShopwareVectorSearch

# Cache leeren  
bin/console cache:clear
```

### 3. Embedding-Modus konfigurieren

Gehe zu **Einstellungen ‚Üí System ‚Üí Plugins ‚Üí Vector Search ‚Üí Konfiguration**

## Modus 1: External Embedding Service üåê

### Wann verwenden?
- Du betreibst bereits einen Embedding-Server
- Du m√∂chtest verschiedene Embedding-Modelle testen
- Du brauchst mehr Kontrolle √ºber die Embedding-Generierung
- Du m√∂chtest Kosten durch lokale Modelle sparen

### Setup:

1. **Embedding Mode**: `External Embedding Service` ausw√§hlen
2. **Embedding Service URL**: URL deines Services eingeben (z.B. `http://localhost:8001`)
3. **OpenAI API Key**: Kann leer bleiben

### Embedding Server starten:
```bash
# OpenAI Embedding Service (aus deinem Projekt)
python openai_embedding_service.py

# Oder anderen Embedding Server nutzen
```

### Testen:
```bash
curl -X POST http://localhost:8001/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "Test Product"}'
```

## Modus 2: Direct OpenAI API ü§ñ

### Wann verwenden?
- Du m√∂chtest keine zus√§tzliche Infrastruktur verwalten
- Du brauchst maximale Zuverl√§ssigkeit (OpenAI SLA)
- Du bevorzugst einfache Konfiguration
- Du hast ein OpenAI Budget verf√ºgbar

### Setup:

1. **Embedding Mode**: `Direct OpenAI API` ausw√§hlen
2. **OpenAI API Key**: Deinen OpenAI API Key eingeben (beginnt mit `sk-...`)
3. **Embedding Service URL**: Kann leer bleiben

### OpenAI API Key erhalten:
1. Registriere dich bei [OpenAI](https://platform.openai.com/)
2. Gehe zu **API Keys** 
3. Erstelle einen neuen API Key
4. Kopiere den Key (beginnt mit `sk-...`)
5. Stelle sicher, dass du Guthaben auf deinem OpenAI Account hast

## Allgemeine Konfiguration

| Einstellung | Beschreibung | Standard | Modi |
|-------------|--------------|----------|------|
| **Embedding Mode** | Auswahl zwischen Service/Direct | `External Service` | Beide |
| **Embedding Service URL** | URL des externen Services | `http://localhost:8001` | Service Mode |
| **OpenAI API Key** | OpenAI API Schl√ºssel | - | Direct Mode |
| **Vector Search aktivieren** | Hauptschalter | `true` | Beide |
| **Batch-Gr√∂√üe** | Produkte pro Batch | `100` | Beide |
| **√Ñhnlichkeitsschwelle** | Minimale √Ñhnlichkeit | `0.7` | Beide |
| **Max. Suchergebnisse** | Maximale Ergebnisse | `20` | Beide |
| **Request Timeout** | Timeout in Sekunden | `30` | Beide |
| **Debug-Logging** | Detaillierte Logs | `false` | Beide |
| **Auto-Reindex** | Auto-Neuindexierung | `true` | Beide |

## Verwendung

### 1. Produkte indexieren
```bash
# Alle Produkte f√ºr Vector Search indexieren
bin/console shopware:vector-search:index
```

### 2. API Endpoints nutzen

**Vector Search:**
```bash
POST /api/_action/vector-search
{
    "query": "rotes Kleid",
    "limit": 10,
    "threshold": 0.7
}
```

**Storefront API:**
```bash
POST /store-api/vector-search  
{
    "query": "smartphone",
    "limit": 5
}
```

## Kostenvergleich

### External Embedding Service üåê
**Kosten:**
- Server-Hosting (z.B. ‚Ç¨5-20/Monat f√ºr VPS)
- Eigene OpenAI API Calls (falls openai_embedding_service.py)
- Oder kostenlos bei lokalen Modellen

**Vorteile:**
- Volle Kontrolle √ºber Modelle
- Potentiell kosteng√ºnstiger bei vielen Anfragen
- Offline-f√§hig mit lokalen Modellen
- Flexibilit√§t bei Modellwechsel

### Direct OpenAI API ü§ñ
**Kosten:**
- text-embedding-ada-002: $0.0001 / 1K tokens
- Beispiel: 1000 Produktbeschreibungen ‚âà $0.05-0.20

**Vorteile:**
- Keine Infrastruktur
- Hochverf√ºgbar (99.9% SLA)
- Automatische Updates
- Einfache Abrechnung

## Monitoring & Troubleshooting

### Logs pr√ºfen
```bash
# Shopware Logs mit Modus-Info
tail -f var/log/dev.log | grep VectorSearch

# Debug-Logging in Plugin-Konfiguration aktivieren
```

### H√§ufige Probleme

**1. "Embedding service URL is not configured" (Service Mode)**
- Pr√ºfe die Service URL in der Konfiguration
- Teste den Service direkt: `curl http://your-service/health`

**2. "OpenAI API Key is not configured" (Direct Mode)**
- Pr√ºfe den API Key in der Konfiguration
- Key muss mit `sk-` beginnen
- Pr√ºfe OpenAI Account Guthaben

**3. "Vector search is disabled"**
- Aktiviere "Vector Search aktivieren" in der Konfiguration

**4. Unterschiedliche Embedding-Qualit√§t**
- Beide Modi nutzen `text-embedding-ada-002` ‚Üí gleiche Qualit√§t
- Bei unterschiedlichen Services: Modell-Kompatibilit√§t pr√ºfen

### Performance Monitoring

**Service Mode √ºberwachen:**
```bash
# Service Health Check
curl http://localhost:8001/health

# Response Times messen
time curl -X POST http://localhost:8001/embed -d '{"text":"test"}'
```

**Direct Mode √ºberwachen:**
- OpenAI Dashboard: [platform.openai.com/usage](https://platform.openai.com/usage)
- Rate Limits beachten (3,000 RPM f√ºr tier 1)

## Migration zwischen Modi

### Von Service zu Direct:
1. OpenAI API Key in Konfiguration eingeben
2. Embedding Mode auf "Direct OpenAI API" √§ndern
3. Konfiguration speichern
4. **Kein Re-Indexing n√∂tig** (gleiche Embeddings)

### Von Direct zu Service:
1. Embedding Server starten
2. Service URL in Konfiguration eingeben
3. Embedding Mode auf "External Embedding Service" √§ndern
4. Konfiguration speichern
5. **Kein Re-Indexing n√∂tig** (gleiche Embeddings)

## Empfehlungen

### F√ºr Development/Testing:
- **External Service** mit lokalem Server
- Kostenlos und flexibel
- Einfaches Debugging

### F√ºr Production (klein):
- **Direct OpenAI API**
- Weniger Infrastruktur
- Zuverl√§ssiger

### F√ºr Production (gro√ü):
- **External Service** mit optimiertem Setup
- Kosteneffizienter bei hohem Volumen
- Mehr Kontrolle √ºber Performance

## Support

Bei Problemen:
1. Debug-Logging aktivieren
2. Logs pr√ºfen (`var/log/dev.log`)
3. Service-spezifische Troubleshooting-Schritte befolgen
4. GitHub Issues erstellen mit Logs und Konfiguration 